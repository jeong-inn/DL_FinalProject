{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcf4e64f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>sub_group</th>\n",
       "      <th>subject</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SLI</td>\n",
       "      <td>A</td>\n",
       "      <td>725</td>\n",
       "      <td>ENNI/SLI/A/725.cha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SLI</td>\n",
       "      <td>A</td>\n",
       "      <td>568</td>\n",
       "      <td>ENNI/SLI/A/568.cha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SLI</td>\n",
       "      <td>A</td>\n",
       "      <td>678</td>\n",
       "      <td>ENNI/SLI/A/678.cha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SLI</td>\n",
       "      <td>A</td>\n",
       "      <td>825</td>\n",
       "      <td>ENNI/SLI/A/825.cha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SLI</td>\n",
       "      <td>A</td>\n",
       "      <td>878</td>\n",
       "      <td>ENNI/SLI/A/878.cha</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  group sub_group  subject            filename\n",
       "0   SLI         A      725  ENNI/SLI/A/725.cha\n",
       "1   SLI         A      568  ENNI/SLI/A/568.cha\n",
       "2   SLI         A      678  ENNI/SLI/A/678.cha\n",
       "3   SLI         A      825  ENNI/SLI/A/825.cha\n",
       "4   SLI         A      878  ENNI/SLI/A/878.cha"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"split/ENNI_train.csv\")\n",
    "dev_df   = pd.read_csv(\"split/ENNI_dev.csv\")\n",
    "test_df  = pd.read_csv(\"split/ENNI_test.csv\")\n",
    "\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e76e5cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 281/281 [00:00<00:00, 1272.78it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 1228.07it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:00<00:00, 1231.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì €ì¥ ì™„ë£Œ: train_ready.csv, dev_ready.csv, test_ready.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------------\n",
    "# 1) CHI ë°œí™” ì¶”ì¶œ í•¨ìˆ˜\n",
    "# ------------------------------\n",
    "def extract_chi_text(path):\n",
    "    texts = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"*CHI:\"):\n",
    "                sent = re.sub(r\"\\x15.+?\\x15\", \"\", line)\n",
    "                sent = sent.replace(\"*CHI:\", \"\").strip()\n",
    "                texts.append(sent)\n",
    "    return \" \".join(texts)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 2) Splitëœ CSV ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "# ------------------------------\n",
    "train_df = pd.read_csv(\"split/ENNI_train.csv\")\n",
    "dev_df   = pd.read_csv(\"split/ENNI_dev.csv\")\n",
    "test_df  = pd.read_csv(\"split/ENNI_test.csv\")\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 3) í…ìŠ¤íŠ¸ & ë¼ë²¨ ì¶”ê°€ í•¨ìˆ˜\n",
    "# ------------------------------\n",
    "def add_text_and_label(df):\n",
    "    texts, labels = [], []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        path = row[\"filename\"]\n",
    "        text = extract_chi_text(path)\n",
    "        texts.append(text)\n",
    "\n",
    "        label = 1 if row[\"group\"] == \"SLI\" else 0\n",
    "        labels.append(label)\n",
    "\n",
    "    df[\"text\"] = texts\n",
    "    df[\"label\"] = labels\n",
    "    return df\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 4) ì ìš© í›„ ì €ì¥\n",
    "# ------------------------------\n",
    "train_df = add_text_and_label(train_df)\n",
    "dev_df   = add_text_and_label(dev_df)\n",
    "test_df  = add_text_and_label(test_df)\n",
    "\n",
    "train_df.to_csv(\"train_ready.csv\", index=False)\n",
    "dev_df.to_csv(\"dev_ready.csv\", index=False)\n",
    "test_df.to_csv(\"test_ready.csv\", index=False)\n",
    "\n",
    "print(\"ì €ì¥ ì™„ë£Œ: train_ready.csv, dev_ready.csv, test_ready.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "964d9e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\në¬¸ì œì (êµ¬ì¡°ì Â·ê¸°ìˆ ì  í•œê³„)\\n1) ìˆœìˆ˜ ë°ì´í„° ì¤€ë¹„ ì½”ë“œ â†’ ML íŒŒì´í”„ë¼ì¸ ìš”ì†Œê°€ ì—†ìŒ\\nì „ì²˜ë¦¬Â·í† í¬ë‚˜ì´ì§•Â·ì„ë² ë”©Â·ëª¨ë¸Â·í‰ê°€ê°€ ì—†ìŒ\\nML ëª¨ë¸ë§ì— í•„ìš”í•œ í•µì‹¬ ëª¨ë“ˆì´ ëª¨ë‘ ë¹ ì ¸ìˆê¸° ë•Œë¬¸ì— í•™ìŠµì— ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ\\n\\n2) ì „ì²˜ë¦¬ í’ˆì§ˆì´ ë§¤ìš° ë‚®ìŒ\\në¬¸ì¥ ë¶„ë¦¬ ì—†ìŒ\\níŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬ ì—†ìŒ\\nì†Œë¬¸ì ë³€í™˜ ì—†ìŒ\\në¶ˆìš©ì–´/ìˆ«ì/ê¸°í˜¸ í•„í„°ë§ ì—†ìŒ\\ní† í° ë‹¨ìœ„ê°€ ì•„ë‹Œ ì „ì²´ ë¬¸ì¥ í•˜ë‚˜ë¡œ ë¬¶ìŒ\\n\\ní…ìŠ¤íŠ¸ ëª¨ë¸ë§ì— ì í•©í•˜ì§€ ì•Šì€ â€œì›ì‹œ í…ìŠ¤íŠ¸â€ë§Œ ì œê³µ\\n\\n3) ëª¨ë¸ë§ì„ ê°€ì •í•˜ì§€ ì•Šì€ êµ¬ì¡°\\nëª¨ë¸ë¡œ ì—°ê²°ë  ìˆ˜ ìˆë„ë¡\\nâ€œí˜•íƒœ(ì „ì²˜ë¦¬ â†’ ì„ë² ë”© â†’ vectorization)â€ êµ¬ì¡°ê°€ ì—†ìŒ\\n\\nraw í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ë‹¤ ë¶™ì´ëŠ” í˜•íƒœë¼ ëª¨ë¸ íŒŒì´í”„ë¼ì¸ê³¼ ì—°ê²° ë¶ˆê°€ëŠ¥.\\n\\n4) ì˜ˆì™¸ ì²˜ë¦¬Â·paddingÂ·token-level ì²˜ë¦¬ ë¶€ì¡±\\n ëª¨ë¸ì€ token sequence êµ¬ì¡°ë¥¼ í•„ìš”ë¡œ í•˜ëŠ”ë°\\nraw stringì´ë¼ LSTM, CNN, transformer ì…ë ¥ìœ¼ë¡œ ì“¸ ìˆ˜ ì—†ìŒ\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ë¬¸ì œì (êµ¬ì¡°ì Â·ê¸°ìˆ ì  í•œê³„)\n",
    "1) ìˆœìˆ˜ ë°ì´í„° ì¤€ë¹„ ì½”ë“œ â†’ ML íŒŒì´í”„ë¼ì¸ ìš”ì†Œê°€ ì—†ìŒ\n",
    "ì „ì²˜ë¦¬Â·í† í¬ë‚˜ì´ì§•Â·ì„ë² ë”©Â·ëª¨ë¸Â·í‰ê°€ê°€ ì—†ìŒ\n",
    "ML ëª¨ë¸ë§ì— í•„ìš”í•œ í•µì‹¬ ëª¨ë“ˆì´ ëª¨ë‘ ë¹ ì ¸ìˆê¸° ë•Œë¬¸ì— í•™ìŠµì— ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ\n",
    "\n",
    "2) ì „ì²˜ë¦¬ í’ˆì§ˆì´ ë§¤ìš° ë‚®ìŒ\n",
    "ë¬¸ì¥ ë¶„ë¦¬ ì—†ìŒ\n",
    "íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬ ì—†ìŒ\n",
    "ì†Œë¬¸ì ë³€í™˜ ì—†ìŒ\n",
    "ë¶ˆìš©ì–´/ìˆ«ì/ê¸°í˜¸ í•„í„°ë§ ì—†ìŒ\n",
    "í† í° ë‹¨ìœ„ê°€ ì•„ë‹Œ ì „ì²´ ë¬¸ì¥ í•˜ë‚˜ë¡œ ë¬¶ìŒ\n",
    "\n",
    "í…ìŠ¤íŠ¸ ëª¨ë¸ë§ì— ì í•©í•˜ì§€ ì•Šì€ â€œì›ì‹œ í…ìŠ¤íŠ¸â€ë§Œ ì œê³µ\n",
    "\n",
    "3) ëª¨ë¸ë§ì„ ê°€ì •í•˜ì§€ ì•Šì€ êµ¬ì¡°\n",
    "ëª¨ë¸ë¡œ ì—°ê²°ë  ìˆ˜ ìˆë„ë¡\n",
    "â€œí˜•íƒœ(ì „ì²˜ë¦¬ â†’ ì„ë² ë”© â†’ vectorization)â€ êµ¬ì¡°ê°€ ì—†ìŒ\n",
    "\n",
    "raw í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ë‹¤ ë¶™ì´ëŠ” í˜•íƒœë¼ ëª¨ë¸ íŒŒì´í”„ë¼ì¸ê³¼ ì—°ê²° ë¶ˆê°€ëŠ¥.\n",
    "\n",
    "4) ì˜ˆì™¸ ì²˜ë¦¬Â·paddingÂ·token-level ì²˜ë¦¬ ë¶€ì¡±\n",
    " ëª¨ë¸ì€ token sequence êµ¬ì¡°ë¥¼ í•„ìš”ë¡œ í•˜ëŠ”ë°\n",
    "raw stringì´ë¼ LSTM, CNN, transformer ì…ë ¥ìœ¼ë¡œ ì“¸ ìˆ˜ ì—†ìŒ\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8df6a4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe ì„ë² ë”© ë¡œë”© ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:06, 60070.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¡œë“œ ì™„ë£Œ! | ë‹¨ì–´ ìˆ˜: 400000\n",
      "LSTM ì¸ì½”ë”© ì¤‘...\n",
      "\n",
      "ë¶„ë¥˜ ì„±ëŠ¥:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       1.0\n",
      "           1       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00       1.0\n",
      "   macro avg       0.00      0.00      0.00       1.0\n",
      "weighted avg       0.00      0.00      0.00       1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeonginn/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jeonginn/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jeonginn/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jeonginn/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jeonginn/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jeonginn/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#ëª¨ë¸ A\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# -------------------------\n",
    "# 1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
    "# -------------------------\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9 ]\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "# -------------------------\n",
    "# 2. í† í¬ë‚˜ì´ì§•\n",
    "# -------------------------\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "# -------------------------\n",
    "# 3. GloVe ë¡œë”©\n",
    "# -------------------------\n",
    "def load_glove(path, dim=100):\n",
    "    print(\"GloVe ì„ë² ë”© ë¡œë”© ì¤‘...\")\n",
    "    embeddings = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f):\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    print(f\"ë¡œë“œ ì™„ë£Œ! | ë‹¨ì–´ ìˆ˜: {len(embeddings)}\")\n",
    "    return embeddings\n",
    "\n",
    "# -------------------------\n",
    "# 4. ë¬¸ì¥ â†’ ì„ë² ë”© ì‹œí€€ìŠ¤ ë³€í™˜\n",
    "# -------------------------\n",
    "def sentence_to_vectors(tokens, glove, dim=100, max_len=30):\n",
    "    vectors = []\n",
    "    for tok in tokens[:max_len]:\n",
    "        if tok in glove:\n",
    "            vectors.append(glove[tok])\n",
    "        else:\n",
    "            vectors.append(np.zeros(dim))\n",
    "    # padding\n",
    "    while len(vectors) < max_len:\n",
    "        vectors.append(np.zeros(dim))\n",
    "    return np.array(vectors)\n",
    "\n",
    "# -------------------------\n",
    "# 5. NumPy LSTM êµ¬í˜„\n",
    "# -------------------------\n",
    "class NumpyLSTM:\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Xavier ì´ˆê¸°í™”\n",
    "        scale = 1.0 / np.sqrt(hidden_dim)\n",
    "\n",
    "        self.W_f = np.random.randn(hidden_dim, input_dim) * scale\n",
    "        self.U_f = np.random.randn(hidden_dim, hidden_dim) * scale\n",
    "        self.b_f = np.zeros((hidden_dim, 1))\n",
    "\n",
    "        self.W_i = np.random.randn(hidden_dim, input_dim) * scale\n",
    "        self.U_i = np.random.randn(hidden_dim, hidden_dim) * scale\n",
    "        self.b_i = np.zeros((hidden_dim, 1))\n",
    "\n",
    "        self.W_c = np.random.randn(hidden_dim, input_dim) * scale\n",
    "        self.U_c = np.random.randn(hidden_dim, hidden_dim) * scale\n",
    "        self.b_c = np.zeros((hidden_dim, 1))\n",
    "\n",
    "        self.W_o = np.random.randn(hidden_dim, input_dim) * scale\n",
    "        self.U_o = np.random.randn(hidden_dim, hidden_dim) * scale\n",
    "        self.b_o = np.zeros((hidden_dim, 1))\n",
    "\n",
    "    def step(self, x_t, h_prev, c_prev):\n",
    "        x_t = x_t.reshape(-1, 1)\n",
    "\n",
    "        f_t = sigmoid(self.W_f @ x_t + self.U_f @ h_prev + self.b_f)\n",
    "        i_t = sigmoid(self.W_i @ x_t + self.U_i @ h_prev + self.b_i)\n",
    "        o_t = sigmoid(self.W_o @ x_t + self.U_o @ h_prev + self.b_o)\n",
    "        c_hat = np.tanh(self.W_c @ x_t + self.U_c @ h_prev + self.b_c)\n",
    "\n",
    "        c_t = f_t * c_prev + i_t * c_hat\n",
    "        h_t = o_t * np.tanh(c_t)\n",
    "\n",
    "        return h_t, c_t\n",
    "    \n",
    "    def forward(self, sequence):\n",
    "        h = np.zeros((self.hidden_dim, 1))\n",
    "        c = np.zeros((self.hidden_dim, 1))\n",
    "\n",
    "        for x_t in sequence:\n",
    "            h, c = self.step(x_t, h, c)\n",
    "        return h.flatten()   # ë§ˆì§€ë§‰ hidden state ë°˜í™˜\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# -------------------------\n",
    "# 6. ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "# -------------------------\n",
    "texts = [\n",
    "    \"This movie is great\",\n",
    "    \"I hated this film\",\n",
    "    \"What a wonderful story\",\n",
    "    \"Terrible acting and bad plot\",\n",
    "]\n",
    "labels = [1, 0, 1, 0]\n",
    "\n",
    "texts = [clean_text(t) for t in texts]\n",
    "tokenized = [tokenize(t) for t in texts]\n",
    "\n",
    "# GloVe ë¡œë”©\n",
    "glove = load_glove(\"glove.6B.100d.txt\", dim=100)\n",
    "\n",
    "# ë¬¸ì¥ â†’ ë²¡í„° ë³€í™˜\n",
    "X = np.array([\n",
    "    sentence_to_vectors(tokens, glove, dim=100, max_len=30)\n",
    "    for tokens in tokenized\n",
    "])\n",
    "\n",
    "# LSTM ì¸ì½”ë”©\n",
    "print(\"LSTM ì¸ì½”ë”© ì¤‘...\")\n",
    "lstm = NumpyLSTM(input_dim=100, hidden_dim=64)\n",
    "\n",
    "X_encoded = np.array([lstm.forward(sentence) for sentence in X])\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, labels, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 7. Scikit-learn ë¶„ë¥˜ê¸° í•™ìŠµ\n",
    "# -------------------------\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "pred = clf.predict(X_test)\n",
    "print(\"\\në¶„ë¥˜ ì„±ëŠ¥:\\n\")\n",
    "print(classification_report(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f298f1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "1) Aë²„ì „ ì„±ëŠ¥ì´ 0ì´ ë‚˜ì˜¨ ì´ìœ \n",
    "Aì½”ë“œëŠ” ë°ëª¨ìš© 4ê°œ ìƒ˜í”Œë¡œë§Œ ëŒë¦° ìƒíƒœ\n",
    "ë°ì´í„°ê°€ 4ê°œ â†’ train/test split ì‹œ test=1ê°œ\n",
    "LSTMì€ í•™ìŠµí•˜ì§€ ì•ŠìŒ(ì„ì˜ ë‚œìˆ˜ë¡œ ê³ ì •ëœ ê°€ì¤‘ì¹˜)\n",
    "SVM/Logisticë„ ì‚¬ì‹¤ìƒ ì˜ë¯¸ ìˆëŠ” í•™ìŠµ ë¶ˆê°€\n",
    "testì— labelì´ 1ê°œë°–ì— ì—†ìœ¼ë¯€ë¡œ precision/recall ê³„ì‚° ë¶ˆê°€\n",
    "ë”°ë¼ì„œ ì „ë¶€ 0ìœ¼ë¡œ ë‚˜ì˜¤ëŠ” ê²ƒ\n",
    "\n",
    "ì¦‰ ë°ì´í„°ê°€ ë„ˆë¬´ ì ì–´ì„œ ì„±ëŠ¥ ê³„ì‚°ì´ ë¶ˆê°€ëŠ¥í•œ ìƒí™©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8fc8728",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading GloVe: 400000it [00:06, 60248.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Train ===\n",
      "Accuracy: 1.0\n",
      "F1: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       228\n",
      "           1       1.00      1.00      1.00        53\n",
      "\n",
      "    accuracy                           1.00       281\n",
      "   macro avg       1.00      1.00      1.00       281\n",
      "weighted avg       1.00      1.00      1.00       281\n",
      "\n",
      "Confusion matrix:\n",
      " [[228   0]\n",
      " [  0  53]]\n",
      "=== Dev ===\n",
      "Accuracy: 0.8\n",
      "F1: 0.5333333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.86      0.87        28\n",
      "           1       0.50      0.57      0.53         7\n",
      "\n",
      "    accuracy                           0.80        35\n",
      "   macro avg       0.69      0.71      0.70        35\n",
      "weighted avg       0.81      0.80      0.80        35\n",
      "\n",
      "Confusion matrix:\n",
      " [[24  4]\n",
      " [ 3  4]]\n",
      "=== Test ===\n",
      "Accuracy: 0.6666666666666666\n",
      "F1: 0.25\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.73      0.79        30\n",
      "           1       0.20      0.33      0.25         6\n",
      "\n",
      "    accuracy                           0.67        36\n",
      "   macro avg       0.52      0.53      0.52        36\n",
      "weighted avg       0.74      0.67      0.70        36\n",
      "\n",
      "Confusion matrix:\n",
      " [[22  8]\n",
      " [ 4  2]]\n",
      "ì €ì¥ ì™„ë£Œ: svm_glove_lstm_svm.pkl, scaler_glove_lstm.pkl, tfidf_glove.pkl\n"
     ]
    }
   ],
   "source": [
    "#ëª¨ë¸ B\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# --------------------------\n",
    "# 1) ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "# --------------------------\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "# --------------------------\n",
    "# 2) GloVe ë¡œë”© (ì´ë¯¸ ë‹¤ìš´ëœ íŒŒì¼ ì‚¬ìš©)\n",
    "# --------------------------\n",
    "def load_glove(path, dim=100):\n",
    "    glove = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, desc=\"Loading GloVe\"):\n",
    "            parts = line.rstrip().split(\" \")\n",
    "            word = parts[0]\n",
    "            vec = np.asarray(parts[1:], dtype=np.float32)\n",
    "            if vec.shape[0] == dim:\n",
    "                glove[word] = vec\n",
    "    return glove\n",
    "\n",
    "# ì˜ˆ: glove = load_glove(\"glove.6B.100d.txt\", dim=100)\n",
    "# EMBDIM = 100\n",
    "\n",
    "# --------------------------\n",
    "# 3) ë¬¸ì¥ â†’ GloVe ì‹œí€€ìŠ¤ ë³€í™˜\n",
    "# --------------------------\n",
    "def sentence_to_seq(tokens, glove, dim=100, max_len=30):\n",
    "    seq = []\n",
    "    for t in tokens[:max_len]:\n",
    "        if t in glove:\n",
    "            seq.append(glove[t])\n",
    "        else:\n",
    "            seq.append(np.zeros(dim, dtype=np.float32))\n",
    "    # padding\n",
    "    while len(seq) < max_len:\n",
    "        seq.append(np.zeros(dim, dtype=np.float32))\n",
    "    return np.array(seq)  # shape: (max_len, dim)\n",
    "\n",
    "# --------------------------\n",
    "# 4) ê°„ë‹¨í•œ NumPy LSTM (forward only)\n",
    "#    - í•™ìŠµí•˜ì§€ ì•Šê³  feature extractorë¡œ ì‚¬ìš©\n",
    "# --------------------------\n",
    "class NumPyLSTMEncoder:\n",
    "    def __init__(self, input_dim, hidden_dim, seed=42):\n",
    "        np.random.seed(seed)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # ì‘ì€ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”\n",
    "        r = 0.1\n",
    "        self.W_f = np.random.randn(hidden_dim, input_dim) * r\n",
    "        self.U_f = np.random.randn(hidden_dim, hidden_dim) * r\n",
    "        self.b_f = np.zeros((hidden_dim,1))\n",
    "\n",
    "        self.W_i = np.random.randn(hidden_dim, input_dim) * r\n",
    "        self.U_i = np.random.randn(hidden_dim, hidden_dim) * r\n",
    "        self.b_i = np.zeros((hidden_dim,1))\n",
    "\n",
    "        self.W_c = np.random.randn(hidden_dim, input_dim) * r\n",
    "        self.U_c = np.random.randn(hidden_dim, hidden_dim) * r\n",
    "        self.b_c = np.zeros((hidden_dim,1))\n",
    "\n",
    "        self.W_o = np.random.randn(hidden_dim, input_dim) * r\n",
    "        self.U_o = np.random.randn(hidden_dim, hidden_dim) * r\n",
    "        self.b_o = np.zeros((hidden_dim,1))\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def forward(self, seq):  # seq: (T, input_dim)\n",
    "        h = np.zeros((self.hidden_dim,1))\n",
    "        c = np.zeros((self.hidden_dim,1))\n",
    "        for t in range(seq.shape[0]):\n",
    "            x = seq[t].reshape(-1,1)\n",
    "            f = self._sigmoid(self.W_f @ x + self.U_f @ h + self.b_f)\n",
    "            i = self._sigmoid(self.W_i @ x + self.U_i @ h + self.b_i)\n",
    "            o = self._sigmoid(self.W_o @ x + self.U_o @ h + self.b_o)\n",
    "            c_hat = np.tanh(self.W_c @ x + self.U_c @ h + self.b_c)\n",
    "            c = f * c + i * c_hat\n",
    "            h = o * np.tanh(c)\n",
    "        return h.flatten()  # (hidden_dim,)\n",
    "\n",
    "# --------------------------\n",
    "# 5) ë³´ì¡° í”¼ì²˜: í‰ê·  ì„ë² ë”©, TF-IDF ê°€ì¤‘ í‰ê· \n",
    "# --------------------------\n",
    "def mean_glove_vector(tokens, glove, dim=100):\n",
    "    vecs = [glove[t] for t in tokens if t in glove]\n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros(dim, dtype=np.float32)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "def tfidf_weighted_avg(texts, glove, dim=100, tfidf=None):\n",
    "    # tfidf: fitted TfidfVectorizer (if None, will fit inside)\n",
    "    if tfidf is None:\n",
    "        tfidf = TfidfVectorizer()\n",
    "        tfidf.fit(texts)\n",
    "    X_tfidf = tfidf.transform(texts)  # sparse\n",
    "    feature_names = tfidf.get_feature_names_out()\n",
    "    # build word->colidx mapping\n",
    "    col_idx = {w:i for i,w in enumerate(feature_names)}\n",
    "    res = []\n",
    "    for doc_idx, txt in enumerate(texts):\n",
    "        tokens = txt.split()\n",
    "        weight_sum = 0.0\n",
    "        vec = np.zeros(dim, dtype=np.float32)\n",
    "        for w in tokens:\n",
    "            if w in col_idx and w in glove:\n",
    "                w_idx = col_idx[w]\n",
    "                weight = X_tfidf[doc_idx, w_idx]\n",
    "                vec += glove[w] * weight\n",
    "                weight_sum += weight\n",
    "        if weight_sum > 0:\n",
    "            vec = vec / weight_sum\n",
    "        res.append(vec)\n",
    "    return np.array(res), tfidf\n",
    "\n",
    "# --------------------------\n",
    "# 6) ì „ì²´ íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜\n",
    "# --------------------------\n",
    "def build_feature_matrix(texts, glove, lstm_encoder, tfidf=None, dim=100, max_len=30):\n",
    "    texts_clean = [clean_text(t) for t in texts]\n",
    "    tokens_list = [tokenize(t) for t in texts_clean]\n",
    "\n",
    "    # mean glove\n",
    "    mean_vectors = np.array([mean_glove_vector(tokens, glove, dim) for tokens in tokens_list])\n",
    "\n",
    "    # tfidf weighted avg (if tfidf provided, reuse)\n",
    "    tfidf_weighted, tfidf = tfidf_weighted_avg(texts_clean, glove, dim, tfidf=tfidf)\n",
    "\n",
    "    # LSTM last hidden\n",
    "    seqs = [sentence_to_seq(tokens, glove, dim=dim, max_len=max_len) for tokens in tokens_list]\n",
    "    lstm_features = np.array([lstm_encoder.forward(seq) for seq in seqs])  # (N, hidden_dim)\n",
    "\n",
    "    # length feature\n",
    "    lengths = np.array([[len(tokens)] for tokens in tokens_list], dtype=np.float32)\n",
    "\n",
    "    # concatenate features: mean_glove | tfidf_weighted | lstm_hidden | length\n",
    "    X = np.hstack([mean_vectors, tfidf_weighted, lstm_features, lengths])\n",
    "    return X, tfidf\n",
    "\n",
    "# --------------------------\n",
    "# 7) ì˜ˆì‹œ: ì‹¤ì œ ë°ì´í„° ë¶ˆëŸ¬ì™€ ì‹¤í–‰\n",
    "# --------------------------\n",
    "# íŒŒì¼ ì´ë¦„ì€ ë„¤ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •\n",
    "train_df = pd.read_csv(\"train_ready.csv\")\n",
    "dev_df   = pd.read_csv(\"dev_ready.csv\")\n",
    "test_df  = pd.read_csv(\"test_ready.csv\")\n",
    "\n",
    "# combine train+dev for tfidf stability (optional)\n",
    "all_train_texts = (train_df[\"text\"].fillna(\"\").tolist() + dev_df[\"text\"].fillna(\"\").tolist())\n",
    "# load glove\n",
    "EMB_DIM = 100\n",
    "glove = load_glove(\"glove.6B.100d.txt\", dim=EMB_DIM)\n",
    "\n",
    "# LSTM encoder\n",
    "lstm_encoder = NumPyLSTMEncoder(input_dim=EMB_DIM, hidden_dim=64, seed=123)\n",
    "\n",
    "# Build features for train\n",
    "train_texts = train_df[\"text\"].fillna(\"\").tolist()\n",
    "dev_texts = dev_df[\"text\"].fillna(\"\").tolist()\n",
    "test_texts = test_df[\"text\"].fillna(\"\").tolist()\n",
    "\n",
    "# fit TF-IDF on train+dev for more stable idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=1)\n",
    "tfidf.fit(all_train_texts)\n",
    "\n",
    "X_train, _ = build_feature_matrix(train_texts, glove, lstm_encoder, tfidf=tfidf, dim=EMB_DIM, max_len=30)\n",
    "X_dev, _   = build_feature_matrix(dev_texts, glove, lstm_encoder, tfidf=tfidf, dim=EMB_DIM, max_len=30)\n",
    "X_test, _  = build_feature_matrix(test_texts, glove, lstm_encoder, tfidf=tfidf, dim=EMB_DIM, max_len=30)\n",
    "\n",
    "y_train = train_df[\"label\"].astype(int).values\n",
    "y_dev   = dev_df[\"label\"].astype(int).values\n",
    "y_test  = test_df[\"label\"].astype(int).values\n",
    "\n",
    "# --------------------------\n",
    "# 8) ìŠ¤ì¼€ì¼ë§ / ì°¨ì›ì¶•ì†Œ (ì„ íƒ)\n",
    "# --------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_dev_s = scaler.transform(X_dev)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "# (ì„ íƒ) PCAë¡œ ì°¨ì› ì¤„ì´ê¸° â€” ì•ˆì •ì„± í•„ìš”í•˜ë©´ ì‚¬ìš©\n",
    "# pca = PCA(n_components=0.95)\n",
    "# X_train_s = pca.fit_transform(X_train_s)\n",
    "# X_dev_s = pca.transform(X_dev_s)\n",
    "# X_test_s = pca.transform(X_test_s)\n",
    "\n",
    "# --------------------------\n",
    "# 9) SVM í•™ìŠµ ë° í‰ê°€\n",
    "# --------------------------\n",
    "svm = SVC(kernel=\"linear\", probability=True, class_weight=\"balanced\")\n",
    "svm.fit(X_train_s, y_train)\n",
    "\n",
    "def evaluate_model(model, X, y, name=\"\"):\n",
    "    preds = model.predict(X)\n",
    "    probs = None\n",
    "    try:\n",
    "        probs = model.predict_proba(X)[:,1]\n",
    "    except:\n",
    "        pass\n",
    "    print(f\"=== {name} ===\")\n",
    "    print(\"Accuracy:\", accuracy_score(y, preds))\n",
    "    print(\"F1:\", f1_score(y, preds, zero_division=0))\n",
    "    print(classification_report(y, preds, zero_division=0))\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y, preds))\n",
    "\n",
    "evaluate_model(svm, X_train_s, y_train, \"Train\")\n",
    "evaluate_model(svm, X_dev_s, y_dev, \"Dev\")\n",
    "evaluate_model(svm, X_test_s, y_test, \"Test\")\n",
    "\n",
    "# --------------------------\n",
    "# 10) ì €ì¥ (ëª¨ë¸, scaler, tfidf)\n",
    "# --------------------------\n",
    "import joblib\n",
    "joblib.dump(svm, \"svm_glove_lstm_svm.pkl\")\n",
    "joblib.dump(scaler, \"scaler_glove_lstm.pkl\")\n",
    "joblib.dump(tfidf, \"tfidf_glove.pkl\")\n",
    "print(\"ì €ì¥ ì™„ë£Œ: svm_glove_lstm_svm.pkl, scaler_glove_lstm.pkl, tfidf_glove.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11f25e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ëª¨ë¸ B ê²°ê³¼ í•´ì„\n",
    "ì „ì²´ ê²°ê³¼ë¥¼ ë³´ë©´ í›ˆë ¨ì€ ì™„ì „ ê³¼ì í•©(accuracy 1.0) \n",
    "-> LSTM ë‹¨ìˆœí™” + SVM ì¡°í•©ì˜ í•œê³„ + ê³¼ì í•© ë•Œë¬¸\n",
    "\n",
    "Dev: 0.80 / F1=0.53\n",
    "â†’ SLI(1 í´ë˜ìŠ¤)ì˜ F1ì´ ë‚®ìŒ\n",
    "\n",
    "Test: 0.67 / F1=0.25\n",
    "â†’ í…ŒìŠ¤íŠ¸ì—ì„œ SLIê°€ ê±°ì˜ ì˜ˆì¸¡ì´ ì•ˆ ë¨\n",
    "â†’ ì‹¤ì§ˆì  ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” ìƒíƒœ\n",
    "\n",
    "\n",
    "B ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ë‚®ì€ ì´ìœ \n",
    "\n",
    "1. NumPy LSTMì´ í•™ìŠµë˜ì§€ ì•Šì€ random encoder â†’ ì˜ë¯¸ ì—†ëŠ” noise\n",
    "hidden_dim=64ì§œë¦¬ ëœë¤ ë²¡í„°ë¥¼ ë’¤ì— ë¶™ì´ëŠ” ì…ˆ â†’ ì„±ëŠ¥ ì €í•˜\n",
    "\n",
    "2. feature dimensionì´ ì§€ë‚˜ì¹˜ê²Œ í¼ (100 + 100 + 64 + 1 = 265ì°¨ì›)\n",
    "TF-IDF weightedê¹Œì§€ í¬í•¨í•˜ë©´ ë” í¼\n",
    "ë°ì´í„° ìƒ˜í”Œ ìˆ˜ ëŒ€ë¹„ feature dimensionì´ ê³¼í•˜ê²Œ í¼ â†’ SVMì´ noiseë¥¼ ì™¸ì›€\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5ac3444",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe loaded: 400000\n",
      "Best Params: {'C': 0.1, 'class_weight': 'balanced', 'gamma': 'scale', 'kernel': 'linear'}\n",
      "\n",
      "=== Dev ì„±ëŠ¥ ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         SLI       0.27      0.57      0.36         7\n",
      "          TD       0.85      0.61      0.71        28\n",
      "\n",
      "    accuracy                           0.60        35\n",
      "   macro avg       0.56      0.59      0.54        35\n",
      "weighted avg       0.73      0.60      0.64        35\n",
      "\n",
      "[[ 4  3]\n",
      " [11 17]]\n",
      "\n",
      "=== Test ì„±ëŠ¥ ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         SLI       0.24      0.67      0.35         6\n",
      "          TD       0.89      0.57      0.69        30\n",
      "\n",
      "    accuracy                           0.58        36\n",
      "   macro avg       0.57      0.62      0.52        36\n",
      "weighted avg       0.78      0.58      0.64        36\n",
      "\n",
      "[[ 4  2]\n",
      " [13 17]]\n"
     ]
    }
   ],
   "source": [
    "#ëª¨ë¸ C\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import re\n",
    "\n",
    "train_df = pd.read_csv(\"ENNI_train.csv\")\n",
    "dev_df   = pd.read_csv(\"ENNI_dev.csv\")\n",
    "test_df  = pd.read_csv(\"ENNI_test.csv\")\n",
    "\n",
    "# í…ìŠ¤íŠ¸ íŒŒì¼(.cha â†’ txt ì „ì²˜ë¦¬ ì™„ë£Œë˜ì–´ ìˆë‹¤ê³  ê°€ì •)\n",
    "train_files = train_df[\"filename\"].tolist()\n",
    "dev_files   = dev_df[\"filename\"].tolist()\n",
    "test_files  = test_df[\"filename\"].tolist()\n",
    "\n",
    "train_y = train_df[\"group\"].tolist()\n",
    "dev_y   = dev_df[\"group\"].tolist()\n",
    "test_y  = test_df[\"group\"].tolist()\n",
    "\n",
    "def load_cha_text(filepath):\n",
    "    lines = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"*CHI\"):  # ì•„ë™ ë°œí™”ë§Œ ì‚¬ìš© (ê¸°ë³¸ ENNI ê´€ë¡€)\n",
    "                clean = re.sub(r\"[^\\w\\s']\", \" \", line)\n",
    "                clean = re.sub(r\"\\s+\", \" \", clean).strip()\n",
    "                lines.append(clean.lower())\n",
    "    return \" \".join(lines)\n",
    "\n",
    "def load_glove(glove_path=\"glove.6B.300d.txt\"):\n",
    "    glove = {}\n",
    "    with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            word = parts[0]\n",
    "            vec = np.array(parts[1:], dtype=\"float32\")\n",
    "            glove[word] = vec\n",
    "    print(\"GloVe loaded:\", len(glove))\n",
    "    return glove\n",
    "\n",
    "glove = load_glove()\n",
    "\n",
    "class NumPyLSTM:\n",
    "    def __init__(self, input_dim=300, hidden_dim=128):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        h = hidden_dim\n",
    "        d = input_dim\n",
    "        \n",
    "        # Xavier ì´ˆê¸°í™”\n",
    "        self.Wf = np.random.randn(h, h + d) / np.sqrt(h + d)\n",
    "        self.Wi = np.random.randn(h, h + d) / np.sqrt(h + d)\n",
    "        self.Wc = np.random.randn(h, h + d) / np.sqrt(h + d)\n",
    "        self.Wo = np.random.randn(h, h + d) / np.sqrt(h + d)\n",
    "        \n",
    "    def step(self, x, h_prev, c_prev):\n",
    "        z = np.concatenate([h_prev, x])  # [hidden_dim + input_dim]\n",
    "\n",
    "        f = self.sigmoid(self.Wf @ z)\n",
    "        i = self.sigmoid(self.Wi @ z)\n",
    "        c_hat = np.tanh(self.Wc @ z)\n",
    "        c = f * c_prev + i * c_hat\n",
    "        o = self.sigmoid(self.Wo @ z)\n",
    "        h = o * np.tanh(c)\n",
    "\n",
    "        return h, c\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def encode(self, seq):\n",
    "        h = np.zeros(self.hidden_dim)\n",
    "        c = np.zeros(self.hidden_dim)\n",
    "        for x in seq:\n",
    "            h, c = self.step(x, h, c)\n",
    "        return h\n",
    "\n",
    "lstm = NumPyLSTM()\n",
    "\n",
    "def sentence_to_glove_seq(text, glove):\n",
    "    seq = []\n",
    "    for w in text.split():\n",
    "        if w in glove:\n",
    "            seq.append(glove[w])\n",
    "    if len(seq) == 0:  # ë¹ˆ ë¬¸ì¥ ë°©ì§€\n",
    "        seq.append(np.zeros(300))\n",
    "    return seq\n",
    "\n",
    "def encode_file(filepath, glove, lstm):\n",
    "    text = load_cha_text(filepath)\n",
    "    seq = sentence_to_glove_seq(text, glove)\n",
    "    return lstm.encode(seq)\n",
    "\n",
    "def encode_dataset(file_list, glove, lstm):\n",
    "    vectors = []\n",
    "    for f in file_list:\n",
    "        vec = encode_file(f, glove, lstm)\n",
    "        vectors.append(vec)\n",
    "    return np.array(vectors)\n",
    "\n",
    "train_vec = encode_dataset(train_files, glove, lstm)\n",
    "dev_vec   = encode_dataset(dev_files, glove, lstm)\n",
    "test_vec  = encode_dataset(test_files, glove, lstm)\n",
    "\n",
    "def train_and_eval(train_x, train_y, dev_x, dev_y, test_x, test_y):\n",
    "\n",
    "    # ğŸ”¹ GridSearchCV íŒŒë¼ë¯¸í„°ì— class_weight='balanced' ì¶”ê°€\n",
    "    params = {\n",
    "        \"C\": [0.1, 1, 10],\n",
    "        \"kernel\": [\"linear\", \"rbf\"],\n",
    "        \"gamma\": [\"scale\", \"auto\"],\n",
    "        \"class_weight\": [\"balanced\"]  \n",
    "    }\n",
    "\n",
    "    svm = SVC()\n",
    "    clf = GridSearchCV(\n",
    "        svm,\n",
    "        params,\n",
    "        cv=3,\n",
    "        scoring='f1_macro',    # ë¶ˆê· í˜•ì¼ ë•Œ macro f1ì´ ë” ì ì ˆ\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    clf.fit(train_x, train_y)\n",
    "\n",
    "    print(\"Best Params:\", clf.best_params_)\n",
    "    best_model = clf.best_estimator_\n",
    "\n",
    "    print(\"\\n=== Dev ì„±ëŠ¥ ===\")\n",
    "    dev_pred = best_model.predict(dev_x)\n",
    "    print(classification_report(dev_y, dev_pred))\n",
    "    print(confusion_matrix(dev_y, dev_pred))\n",
    "\n",
    "    print(\"\\n=== Test ì„±ëŠ¥ ===\")\n",
    "    test_pred = best_model.predict(test_x)\n",
    "    print(classification_report(test_y, test_pred))\n",
    "    print(confusion_matrix(test_y, test_pred))\n",
    "\n",
    "    return best_model\n",
    "clf = train_and_eval(train_vec, train_y, dev_vec, dev_y, test_vec, test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ef25b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dev: SLI recall 57%\n",
    "Test: SLI recall 67%\n",
    "\n",
    "\n",
    "TD ì„±ëŠ¥ì´ ë–¨ì–´ì§\n",
    "TD recallì´ 100 â†’ 57ë¡œ ê°ì†Œ\n",
    "\n",
    "Accuracy\n",
    "Dev 0.74\n",
    "Test 0.83\n",
    "â¡ balanced SVM ì¹˜ê³  ì–‘í˜¸\n",
    "\n",
    "ì¦‰, ëª¨ë¸ì´ SLIë¥¼ ì¡ëŠ” ëŒ€ì‹  TDë¥¼ SLIë¡œ ë” ì˜¤íŒí•˜ëŠ” ë°©í–¥\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "281b8f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading GloVe: 400000it [00:17, 22498.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe loaded: 400000\n",
      "ìŠ¤ì¼€ì¼ëŸ¬ ì ìš© ì¤‘...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "\n",
      "Best Params: {'C': 3, 'class_weight': 'balanced', 'gamma': 'scale', 'kernel': 'linear'}\n",
      "\n",
      "=== Dev ì„±ëŠ¥ ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         SLI       0.00      0.00      0.00         7\n",
      "          TD       0.73      0.68      0.70        28\n",
      "\n",
      "    accuracy                           0.54        35\n",
      "   macro avg       0.37      0.34      0.35        35\n",
      "weighted avg       0.58      0.54      0.56        35\n",
      "\n",
      "[[ 0  7]\n",
      " [ 9 19]]\n",
      "\n",
      "=== Test ì„±ëŠ¥ ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         SLI       0.29      0.33      0.31         6\n",
      "          TD       0.86      0.83      0.85        30\n",
      "\n",
      "    accuracy                           0.75        36\n",
      "   macro avg       0.57      0.58      0.58        36\n",
      "weighted avg       0.77      0.75      0.76        36\n",
      "\n",
      "[[ 2  4]\n",
      " [ 5 25]]\n"
     ]
    }
   ],
   "source": [
    "#ëª¨ë¸ D\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# =========================\n",
    "# 1. GloVe ë¡œë“œ\n",
    "# =========================\n",
    "def load_glove(glove_path):\n",
    "    embeddings = {}\n",
    "    with open(glove_path, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in tqdm(f, desc=\"Loading GloVe\"):\n",
    "            values = line.rstrip().split(\" \")\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2. ì „ì²˜ë¦¬ ë° ë¬¸ì¥ â†’ ì„ë² ë”© ë³€í™˜\n",
    "# =========================\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    return text.split()\n",
    "\n",
    "def sentence_to_embedding(sentence, glove, dim=300):\n",
    "    tokens = preprocess(sentence)\n",
    "    vecs = [glove[t] for t in tokens if t in glove]\n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros((1, dim))\n",
    "    return np.array(vecs)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3. NumPy LSTM Encoder\n",
    "# =========================\n",
    "class NumpyLSTM:\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.Wf = np.random.randn(hidden_dim, input_dim + hidden_dim) * 0.1\n",
    "        self.bf = np.zeros((hidden_dim, 1))\n",
    "\n",
    "        self.Wi = np.random.randn(hidden_dim, input_dim + hidden_dim) * 0.1\n",
    "        self.bi = np.zeros((hidden_dim, 1))\n",
    "\n",
    "        self.Wc = np.random.randn(hidden_dim, input_dim + hidden_dim) * 0.1\n",
    "        self.bc = np.zeros((hidden_dim, 1))\n",
    "\n",
    "        self.Wo = np.random.randn(hidden_dim, input_dim + hidden_dim) * 0.1\n",
    "        self.bo = np.zeros((hidden_dim, 1))\n",
    "\n",
    "    def step(self, x, h_prev, c_prev):\n",
    "        concat = np.vstack((h_prev, x))\n",
    "\n",
    "        f = self.sigmoid(self.Wf @ concat + self.bf)\n",
    "        i = self.sigmoid(self.Wi @ concat + self.bi)\n",
    "        c_hat = np.tanh(self.Wc @ concat + self.bc)\n",
    "        c = f * c_prev + i * c_hat\n",
    "        o = self.sigmoid(self.Wo @ concat + self.bo)\n",
    "        h = o * np.tanh(c)\n",
    "        return h, c\n",
    "\n",
    "    def encode(self, sequence):\n",
    "        h = np.zeros((self.hidden_dim, 1))\n",
    "        c = np.zeros((self.hidden_dim, 1))\n",
    "        for word_vec in sequence:\n",
    "            x = word_vec.reshape(-1, 1)\n",
    "            h, c = self.step(x, h, c)\n",
    "        return h.reshape(-1)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4. Dataset â†’ ë²¡í„°ë¡œ ë³€í™˜\n",
    "# =========================\n",
    "def encode_dataset(texts, glove, lstm):\n",
    "    vectors = []\n",
    "    for sent in texts:\n",
    "        seq = sentence_to_embedding(sent, glove)\n",
    "        h = lstm.encode(seq)\n",
    "        vectors.append(h)\n",
    "    return np.array(vectors)\n",
    "\n",
    "# --------------------------------------------\n",
    "# SVM í•™ìŠµ í•¨ìˆ˜ (ê°œì„ ëœ version a)\n",
    "# --------------------------------------------\n",
    "def train_svm_balanced(train_x, train_y, dev_x, dev_y, test_x, test_y):\n",
    "    print(\"ìŠ¤ì¼€ì¼ëŸ¬ ì ìš© ì¤‘...\")\n",
    "    scaler = StandardScaler()\n",
    "    train_x = scaler.fit_transform(train_x)\n",
    "    dev_x   = scaler.transform(dev_x)\n",
    "    test_x  = scaler.transform(test_x)\n",
    "\n",
    "    # GridSearch ë²”ìœ„ í™•ì¥ (collapse ë°©ì§€)\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 3, 10],\n",
    "        'kernel': ['linear'],\n",
    "        'gamma': ['scale'],\n",
    "        'class_weight': ['balanced']\n",
    "    }\n",
    "\n",
    "    svm = SVC()\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        svm,\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        scoring='f1_macro',\n",
    "        verbose=1,\n",
    "        n_jobs=1\n",
    "    )\n",
    "\n",
    "    grid.fit(train_x, train_y)\n",
    "\n",
    "    print(\"\\nBest Params:\", grid.best_params_)\n",
    "\n",
    "    best_svm = grid.best_estimator_\n",
    "\n",
    "    # ----------------------------\n",
    "    # Dev ì„±ëŠ¥\n",
    "    # ----------------------------\n",
    "    print(\"\\n=== Dev ì„±ëŠ¥ ===\")\n",
    "    pred_dev = best_svm.predict(dev_x)\n",
    "    print(classification_report(dev_y, pred_dev))\n",
    "    print(confusion_matrix(dev_y, pred_dev))\n",
    "\n",
    "    # ----------------------------\n",
    "    # Test ì„±ëŠ¥\n",
    "    # ----------------------------\n",
    "    print(\"\\n=== Test ì„±ëŠ¥ ===\")\n",
    "    pred_test = best_svm.predict(test_x)\n",
    "    print(classification_report(test_y, pred_test))\n",
    "    print(confusion_matrix(test_y, pred_test))\n",
    "\n",
    "    return best_svm, scaler, grid.best_params_\n",
    "\n",
    "# =========================\n",
    "# 6. ì‹¤ì œ ì‹¤í–‰\n",
    "# =========================\n",
    "\n",
    "glove_path = \"glove.6B.300d.txt\"\n",
    "glove = load_glove(glove_path)\n",
    "print(\"GloVe loaded:\", len(glove))\n",
    "\n",
    "# LSTM ì¸ì½”ë”\n",
    "lstm = NumpyLSTM(input_dim=300, hidden_dim=128)\n",
    "\n",
    "# í…ìŠ¤íŠ¸ â†’ ë²¡í„°\n",
    "train_vec = encode_dataset(train_texts, glove, lstm)\n",
    "dev_vec   = encode_dataset(dev_texts, glove, lstm)\n",
    "test_vec  = encode_dataset(test_texts, glove, lstm)\n",
    "\n",
    "# SVM í•™ìŠµ ì‹¤í–‰\n",
    "svm_model, scaler, params = train_svm_balanced(\n",
    "    train_vec, train_y,\n",
    "    dev_vec, dev_y,\n",
    "    test_vec, test_y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e143ff55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSLI (ì†Œìˆ˜ í´ë˜ìŠ¤)\\nRecall: 0%\\nPrecision: 0%\\n\\nConfusion matrix:\\nSLI 7ê°œ â†’ ì „ë¶€ TDë¡œ ë¶„ë¥˜ë¨ (0/7 íƒì§€)\\n\\nì¦‰, Devì—ì„œëŠ” SLIë¥¼ ì „í˜€ ëª» ì¡ìŒ.\\nì™œëƒë©´ trainì—ì„œëŠ” SLIë¥¼ ì–´ëŠ ì •ë„ í•™ìŠµí–ˆì§€ë§Œ dev ë¶„í¬ì—ì„œ íŒ¨í„´ ì°¨ì´ê°€ ìˆì–´ì„œ ë¦¬ì½œì´ 0ìœ¼ë¡œ ë–¨ì–´ì§„ ê²ƒ\\n\\nTD (ë‹¤ìˆ˜ í´ë˜ìŠ¤)\\nRecall: 68%\\nF1: 0.70\\n SLIë¥¼ í•˜ë‚˜ë„ ëª» ì¡ëŠ” êµ¬ì¡°ì´ê¸° ë•Œë¬¸ì— Dev ì„±ëŠ¥ì€ ë§¤ìš° ë‚®ìŒ (acc=0.54)\\n \\nTestì—ì„œëŠ” TDë„, SLIë„ ì¼ì • ìˆ˜ì¤€ ì´ìƒ ì¡ìŒ\\níŠ¹íˆ accuracy 75%ëŠ” ê½¤ ì¢‹ì€ í¸\\n\\nDevì™€ Test ê°„ì˜ ê²°ê³¼ê°€ ë„ˆë¬´ ë‹¤ë¦„ â†’ ëª¨ë¸ì´ êµ‰ì¥íˆ ë¶ˆì•ˆì •í•¨\\n\\nLSTMì´ ì™„ì „í•œ í•™ìŠµì´ ì•„ë‹ˆë¼ ëœë¤ ì´ˆê¸°í™”ë¼ ì¬í˜„ì„± ìì²´ê°€ ë‚®ìŒ\\n(ì§„ì§œ í•™ìŠµëœ LSTMì´ ì•„ë‹ˆë¼ íŠ¹ì§• ì¶”ì¶œì´ ëœë¤ ê¸°ë°˜)\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DEV :\n",
    "SLI (ì†Œìˆ˜ í´ë˜ìŠ¤)\n",
    "Recall: 0%\n",
    "Precision: 0%\n",
    "\n",
    "Confusion matrix:\n",
    "SLI 7ê°œ â†’ ì „ë¶€ TDë¡œ ë¶„ë¥˜ë¨ (0/7 íƒì§€)\n",
    "\n",
    "ì¦‰, Devì—ì„œëŠ” SLIë¥¼ ì „í˜€ ëª» ì¡ìŒ.\n",
    "ì™œëƒë©´ trainì—ì„œëŠ” SLIë¥¼ ì–´ëŠ ì •ë„ í•™ìŠµí–ˆì§€ë§Œ dev ë¶„í¬ì—ì„œ íŒ¨í„´ ì°¨ì´ê°€ ìˆì–´ì„œ ë¦¬ì½œì´ 0ìœ¼ë¡œ ë–¨ì–´ì§„ ê²ƒ\n",
    "\n",
    "TD (ë‹¤ìˆ˜ í´ë˜ìŠ¤)\n",
    "Recall: 68%\n",
    "F1: 0.70\n",
    " SLIë¥¼ í•˜ë‚˜ë„ ëª» ì¡ëŠ” êµ¬ì¡°ì´ê¸° ë•Œë¬¸ì— Dev ì„±ëŠ¥ì€ ë§¤ìš° ë‚®ìŒ (acc=0.54)\n",
    "\n",
    "Test :\n",
    "TDë„, SLIë„ ì¼ì • ìˆ˜ì¤€ ì´ìƒ ì¡ìŒ\n",
    "íŠ¹íˆ accuracy 75%ëŠ” ê½¤ ì¢‹ì€ í¸\n",
    "\n",
    "\n",
    "Devì™€ Test ê°„ì˜ ê²°ê³¼ê°€ ë„ˆë¬´ ë‹¤ë¦„ â†’ ëª¨ë¸ì´ êµ‰ì¥íˆ ë¶ˆì•ˆì •í•¨\n",
    "\n",
    "LSTMì´ ì™„ì „í•œ í•™ìŠµì´ ì•„ë‹ˆë¼ ëœë¤ ì´ˆê¸°í™”ë¼ ì¬í˜„ì„± ìì²´ê°€ ë‚®ìŒ\n",
    "(ì§„ì§œ í•™ìŠµëœ LSTMì´ ì•„ë‹ˆë¼ íŠ¹ì§• ì¶”ì¶œì´ ëœë¤ ê¸°ë°˜)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0746d0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400000/400000 [00:17<00:00, 22526.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe loaded: 400000\n",
      "ìŠ¤ì¼€ì¼ëŸ¬ ì ìš© ì¤‘...\n",
      "\n",
      "Best Params: {'C': 50, 'class_weight': 'balanced', 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "=== Dev ì„±ëŠ¥ ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         SLI       0.00      0.00      0.00         7\n",
      "          TD       0.78      0.89      0.83        28\n",
      "\n",
      "    accuracy                           0.71        35\n",
      "   macro avg       0.39      0.45      0.42        35\n",
      "weighted avg       0.62      0.71      0.67        35\n",
      "\n",
      "[[ 0  7]\n",
      " [ 3 25]]\n",
      "\n",
      "=== Test ì„±ëŠ¥ ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         SLI       0.43      0.50      0.46         6\n",
      "          TD       0.90      0.87      0.88        30\n",
      "\n",
      "    accuracy                           0.81        36\n",
      "   macro avg       0.66      0.68      0.67        36\n",
      "weighted avg       0.82      0.81      0.81        36\n",
      "\n",
      "[[ 3  3]\n",
      " [ 4 26]]\n"
     ]
    }
   ],
   "source": [
    "#ëª¨ë¸ E\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 1. GloVe ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "# ================================\n",
    "def load_glove(path):\n",
    "    glove = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, total=400000):\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vec = np.asarray(values[1:], dtype='float32')\n",
    "            glove[word] = vec\n",
    "    print(\"GloVe loaded:\", len(glove))\n",
    "    return glove\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 2. ê°„ë‹¨í•œ NumPy LSTM êµ¬í˜„\n",
    "# ================================\n",
    "class NumpyLSTM:\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.W = np.random.randn(input_dim + hidden_dim, 4 * hidden_dim) * 0.1\n",
    "        self.b = np.zeros((4 * hidden_dim,))\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = np.zeros((self.hidden_dim,))\n",
    "        c = np.zeros((self.hidden_dim,))\n",
    "        for t in range(x.shape[0]):\n",
    "            xt = x[t]\n",
    "            concat = np.concatenate([h, xt])\n",
    "            gates = concat @ self.W + self.b\n",
    "\n",
    "            i = 1 / (1 + np.exp(-gates[:self.hidden_dim]))         # input gate\n",
    "            f = 1 / (1 + np.exp(-gates[self.hidden_dim:2*self.hidden_dim]))  # forget gate\n",
    "            o = 1 / (1 + np.exp(-gates[2*self.hidden_dim:3*self.hidden_dim])) # output gate\n",
    "            g = np.tanh(gates[3*self.hidden_dim:])                          # candidate\n",
    "\n",
    "            c = f * c + i * g\n",
    "            h = o * np.tanh(c)\n",
    "\n",
    "        return h\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 3. í…ìŠ¤íŠ¸ â†’ GloVe â†’ LSTM ì¸ì½”ë”©\n",
    "# ================================\n",
    "def encode_sentence(text, glove, lstm, max_len=50):\n",
    "    tokens = text.lower().split()\n",
    "    vecs = []\n",
    "\n",
    "    for tok in tokens[:max_len]:\n",
    "        if tok in glove:\n",
    "            vecs.append(glove[tok])\n",
    "        else:\n",
    "            vecs.append(np.zeros((300,)))\n",
    "\n",
    "    if len(vecs) == 0:\n",
    "        vecs.append(np.zeros((300,)))\n",
    "\n",
    "    return lstm.forward(np.array(vecs))\n",
    "\n",
    "\n",
    "def encode_dataset(texts, glove, lstm):\n",
    "    return np.array([encode_sentence(t, glove, lstm) for t in texts])\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 4. SVM(RBF ì¤‘ì‹¬) + class_weight=balanced íƒìƒ‰\n",
    "# ================================\n",
    "def train_svm_rbf(train_x, train_y, dev_x, dev_y, test_x, test_y):\n",
    "\n",
    "    print(\"ìŠ¤ì¼€ì¼ëŸ¬ ì ìš© ì¤‘...\")\n",
    "    scaler = StandardScaler()\n",
    "    train_x = scaler.fit_transform(train_x)\n",
    "    dev_x   = scaler.transform(dev_x)\n",
    "    test_x  = scaler.transform(test_x)\n",
    "\n",
    "    param_grid = {\n",
    "        \"C\":     [0.1, 1, 10, 50],\n",
    "        \"gamma\": [\"scale\", 0.01, 0.001],\n",
    "        \"kernel\": [\"rbf\"],\n",
    "        \"class_weight\": [\"balanced\"]\n",
    "    }\n",
    "\n",
    "    svm = SVC()\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        svm,\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid.fit(train_x, train_y)\n",
    "\n",
    "    best_svm = grid.best_estimator_\n",
    "\n",
    "    print(\"\\nBest Params:\", grid.best_params_)\n",
    "\n",
    "    print(\"\\n=== Dev ì„±ëŠ¥ ===\")\n",
    "    pred_dev = best_svm.predict(dev_x)\n",
    "    print(classification_report(dev_y, pred_dev))\n",
    "    print(confusion_matrix(dev_y, pred_dev))\n",
    "\n",
    "    print(\"\\n=== Test ì„±ëŠ¥ ===\")\n",
    "    pred_test = best_svm.predict(test_x)\n",
    "    print(classification_report(test_y, pred_test))\n",
    "    print(confusion_matrix(test_y, pred_test))\n",
    "\n",
    "    return best_svm, scaler, grid.best_params_\n",
    "\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 5. ì‹¤í–‰ íŒŒíŠ¸\n",
    "# ================================\n",
    "# ì˜ˆì‹œ: ì‹¤ì œ ì‚¬ìš© ì‹œ ì•„ë˜ ë³€ìˆ˜ë§Œ ë°ì´í„°ì…‹ì— ë§ê²Œ êµì²´\n",
    "glove_path = \"glove.6B.300d.txt\"\n",
    "\n",
    "glove = load_glove(glove_path)\n",
    "lstm = NumpyLSTM(input_dim=300, hidden_dim=128)\n",
    "\n",
    "# ì—¬ëŸ¬ë¶„ì˜ ë¶„í• ëœ í…ìŠ¤íŠ¸/ë ˆì´ë¸” ì‚¬ìš©\n",
    "# train_texts, train_y\n",
    "# dev_texts, dev_y\n",
    "# test_texts, test_y\n",
    "\n",
    "train_vec = encode_dataset(train_texts, glove, lstm)\n",
    "dev_vec   = encode_dataset(dev_texts, glove, lstm)\n",
    "test_vec  = encode_dataset(test_texts, glove, lstm)\n",
    "\n",
    "svm_model, scaler, best_params = train_svm_rbf(\n",
    "    train_vec, train_y,\n",
    "    dev_vec, dev_y,\n",
    "    test_vec, test_y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9f1261",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dev :\n",
    "SLI\n",
    "Recall = 0% (0/7)\n",
    "Precision = 0\n",
    "Devì—ì„œëŠ” SLIë¥¼ ì „í˜€ ì¡ì§€ ëª»í•¨.\n",
    "\n",
    "TD\n",
    "Recall = 89%\n",
    "Precision = 78%\n",
    "F1 = 83%\n",
    "\n",
    "â†’ Dev ì •í™•ë„ 0.71ì€ ì¢‹ì•„ ë³´ì´ì§€ë§Œ\n",
    "SLIë¥¼ 1ê°œë„ ëª» ë§ì¶”ê¸° ë•Œë¬¸ì— ëª¨ë¸ ì„±ëŠ¥ì´ ë†’ì€ ê²Œ ì•„ë‹˜\n",
    "\n",
    "Test :\n",
    "SLI\n",
    "Recall = 50% (3/6 ë§ì¶¤)\n",
    "Precision = 43%\n",
    "F1 = 0.46\n",
    "\n",
    "SLIë¥¼ ì ˆë°˜ì´ë‚˜ ì¡ì€ ê²ƒì€ ì§€ê¸ˆê¹Œì§€ ëª¨ë¸ ì¤‘ ìµœê³  ì„±ëŠ¥\n",
    "\n",
    "TD\n",
    "Recall = 87%\n",
    "Precision = 90%\n",
    "F1 = 0.88\n",
    "\n",
    "ë§¤ìš° ì•ˆì •ì ì¸ TD ì„±ëŠ¥ ìœ ì§€.\n",
    "Test ì „ì²´ ì •í™•ë„ = 0.81\n",
    "\n",
    "ì›ì¸:\n",
    "1) í´ë˜ìŠ¤ ë¶ˆê· í˜• (SLIê°€ ë„ˆë¬´ ì ìŒ)\n",
    "ê°ê° train/dev/testì—ì„œ SLIê°€ 6~7ê°œ ìˆ˜ì¤€\n",
    "SVM + ì‘ì€ ìƒ˜í”Œ â†’ ê±°ì˜ ë¶ˆê°€ëŠ¥\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4d6c05a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400000/400000 [00:17<00:00, 22911.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe loaded: 400000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "train_svm_rbf() takes 2 positional arguments but 6 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w5/1h0crrlj5hx238pnm9g4gcgw0000gn/T/ipykernel_72955/3227148557.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0mtest_vec\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mencode_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m svm_model, scaler, best_params = train_svm_rbf(\n\u001b[0m\u001b[1;32m    150\u001b[0m     \u001b[0mtrain_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mdev_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: train_svm_rbf() takes 2 positional arguments but 6 were given"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 1. GloVe ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "# ================================\n",
    "def load_glove(path):\n",
    "    glove = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, total=400000):\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vec = np.asarray(values[1:], dtype='float32')\n",
    "            glove[word] = vec\n",
    "    print(\"GloVe loaded:\", len(glove))\n",
    "    return glove\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 2. ê°„ë‹¨í•œ NumPy LSTM êµ¬í˜„\n",
    "# ================================\n",
    "class NumpyLSTM:\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.W = np.random.randn(input_dim + hidden_dim, 4 * hidden_dim) * 0.1\n",
    "        self.b = np.zeros((4 * hidden_dim,))\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = np.zeros((self.hidden_dim,))\n",
    "        c = np.zeros((self.hidden_dim,))\n",
    "        for t in range(x.shape[0]):\n",
    "            xt = x[t]\n",
    "            concat = np.concatenate([h, xt])\n",
    "            gates = concat @ self.W + self.b\n",
    "\n",
    "            i = 1 / (1 + np.exp(-gates[:self.hidden_dim]))         # input gate\n",
    "            f = 1 / (1 + np.exp(-gates[self.hidden_dim:2*self.hidden_dim]))  # forget gate\n",
    "            o = 1 / (1 + np.exp(-gates[2*self.hidden_dim:3*self.hidden_dim])) # output gate\n",
    "            g = np.tanh(gates[3*self.hidden_dim:])                          # candidate\n",
    "\n",
    "            c = f * c + i * g\n",
    "            h = o * np.tanh(c)\n",
    "\n",
    "        return h\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 3. í…ìŠ¤íŠ¸ â†’ GloVe â†’ LSTM ì¸ì½”ë”©\n",
    "# ================================\n",
    "def encode_sentence(text, glove, lstm, max_len=50):\n",
    "    tokens = text.lower().split()\n",
    "    vecs = []\n",
    "\n",
    "    for tok in tokens[:max_len]:\n",
    "        if tok in glove:\n",
    "            vecs.append(glove[tok])\n",
    "        else:\n",
    "            vecs.append(np.zeros((300,)))\n",
    "\n",
    "    if len(vecs) == 0:\n",
    "        vecs.append(np.zeros((300,)))\n",
    "\n",
    "    return lstm.forward(np.array(vecs))\n",
    "\n",
    "\n",
    "def encode_dataset(texts, glove, lstm):\n",
    "    return np.array([encode_sentence(t, glove, lstm) for t in texts])\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 4. SVM(RBF ì¤‘ì‹¬) + class_weight=balanced íƒìƒ‰\n",
    "# ================================\n",
    "\n",
    "def train_svm_smote(train_x, train_y, dev_x, dev_y, test_x, test_y):\n",
    "\n",
    "    print(\"ìŠ¤ì¼€ì¼ëŸ¬ ì ìš© ì¤‘...\")\n",
    "    scaler = StandardScaler()\n",
    "    train_x = scaler.fit_transform(train_x)\n",
    "    dev_x   = scaler.transform(dev_x)\n",
    "    test_x  = scaler.transform(test_x)\n",
    "\n",
    "    # 1ï¸âƒ£ SMOTE ì ìš© (SLI ë°ì´í„° ì¦ê°€)\n",
    "    print(\"SMOTE ì ìš© ì¤‘...\")\n",
    "    smote = SMOTE(k_neighbors=3, random_state=42)\n",
    "    train_x, train_y = smote.fit_resample(train_x, train_y)\n",
    "\n",
    "    print(\"SMOTE í›„ í´ë˜ìŠ¤ ë¶„í¬:\", np.bincount(train_y))\n",
    "\n",
    "    # 2ï¸âƒ£ íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ\n",
    "    param_grid = {\n",
    "        \"C\": [0.1, 1, 10, 50],\n",
    "        \"gamma\": [\"scale\", 0.01, 0.001],\n",
    "        \"kernel\": [\"rbf\"],\n",
    "        \"class_weight\": [\"balanced\"]\n",
    "    }\n",
    "\n",
    "    svm = SVC()\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        svm,\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid.fit(train_x, train_y)\n",
    "\n",
    "    best_svm = grid.best_estimator_\n",
    "\n",
    "    print(\"\\nBest Params:\", grid.best_params_)\n",
    "\n",
    "    # 3ï¸âƒ£ Dev ì„±ëŠ¥ í™•ì¸\n",
    "    print(\"\\n=== Dev ì„±ëŠ¥ ===\")\n",
    "    pred_dev = best_svm.predict(dev_x)\n",
    "    print(classification_report(dev_y, pred_dev))\n",
    "    print(confusion_matrix(dev_y, pred_dev))\n",
    "\n",
    "    # 4ï¸âƒ£ Test ì„±ëŠ¥ í™•ì¸\n",
    "    print(\"\\n=== Test ì„±ëŠ¥ ===\")\n",
    "    pred_test = best_svm.predict(test_x)\n",
    "    print(classification_report(test_y, pred_test))\n",
    "    print(confusion_matrix(test_y, pred_test))\n",
    "\n",
    "    return best_svm, scaler, grid.best_params_\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 5. ì‹¤í–‰ íŒŒíŠ¸\n",
    "# ================================\n",
    "# ì˜ˆì‹œ: ì‹¤ì œ ì‚¬ìš© ì‹œ ì•„ë˜ ë³€ìˆ˜ë§Œ ë°ì´í„°ì…‹ì— ë§ê²Œ êµì²´\n",
    "glove_path = \"glove.6B.300d.txt\"\n",
    "\n",
    "glove = load_glove(glove_path)\n",
    "lstm = NumpyLSTM(input_dim=300, hidden_dim=128)\n",
    "\n",
    "# ì—¬ëŸ¬ë¶„ì˜ ë¶„í• ëœ í…ìŠ¤íŠ¸/ë ˆì´ë¸” ì‚¬ìš©\n",
    "# train_texts, train_y\n",
    "# dev_texts, dev_y\n",
    "# test_texts, test_y\n",
    "\n",
    "train_vec = encode_dataset(train_texts, glove, lstm)\n",
    "dev_vec   = encode_dataset(dev_texts, glove, lstm)\n",
    "test_vec  = encode_dataset(test_texts, glove, lstm)\n",
    "\n",
    "svm_model, scaler, best_params = train_svm_rbf(\n",
    "    train_vec, train_y,\n",
    "    dev_vec, dev_y,\n",
    "    test_vec, test_y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76911b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
